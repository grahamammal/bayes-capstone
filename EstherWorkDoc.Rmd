---
title: "EstherWorkDoc"
author: "Esther Swehla"
date: "4/24/2020"
output: html_document
bibliography: Library.bib
---

Model specification is an element of statistics whose importance is often glossed over. The choice of model can hugely impact the results, and classical methods offer limited guidance on the best process for accounting for the uncertainty this creates. Unstructured searches and checks for the best model specification can lead to incorrect inferences, fragile reported findings, and publication bias [@Fragoso]. Bayesian Model Averaging (BMA) offers an alternative practice that helps ensure findings are robust to a variety of model specifications. At its simplest level, BMA assigns priors to potential model specifications and then caluclates posterior distributions for the model itself, in addition to the coefficients within the specification. This is thus an extension of Bayesian methodology that focuses solely on coefficient estimation. 

Let us begin by considering a matrix $X$ of all the $n \times p$ potential independent variables to predict a response variable $Y$. A standard linear analysis would assume $Y = X \beta + \epsilon$, where $\beta$ is a coefficient matrix and $\epsilon$ ~ $N(0, \sigma^2)$. In many cases, we are still left with ambiguity about which of the $q=2^p$ model specifications from the model space ${M_1, M_2, ... M_q}$ is best. BMA incorporates this uncertainty into the process rather than ignoring it and claiming that the final model is the only option. This leads to greater flexibility in the inferences of the end results. 

We can now assign a prior probability to each of the model specifications $M_k$, as well as the model parameters ($\beta, \sigma^2$). We assume $M_k$ ~ $\pi(M_k)$ and $\sigma^2 | M_k$ ~ $\pi(\sigma^2|M_k)$, $\beta_{\omega} |\sigma^2, M_k$ ~ $\pi(\beta_{\omega} |\sigma^2, M_k )$. In this last assumption, $\Omega$ represents a vector ${\omega_1, \omega_2, ... \omega_p}$ that is populated with zeros and ones indicating the exlcusion or inclusion of variables in model $M_k$. 

If we assume that $Y|\beta_{\omega}, \sigma^2, M_k$ ~ $N(X_{\omega}\beta_{\omega}, \sigma^2)$ then we know that the marginal distribution of the data under model $M_k$ follows $p(Y|M_k) = \int \int p(Y|\beta_{\omega}, \sigma^2, M_k) \pi(\beta_{\omega}|\sigma^2, M_k) \pi(\sigma^2|M_k) d\beta_{\omega} d\sigma^2$. 

Then, the posterior probability of model specification $M_k$  is $p(M_k | Y) = \frac{L(M_k | Y) \pi(M_k)}{\sum_{k=0}^q p(Y|M_k)\pi(M_k)}$. 